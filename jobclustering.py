# -*- coding: utf-8 -*-
"""jobclustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SiwgIb5U06_Dzy8SFoV4chjPYl27zLr9
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page} -> {url}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)

    return pd.DataFrame(jobs_list)

if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    df_jobs.to_csv("karkidi_jobs.csv", index=False)
    print(df_jobs.head())

def clean_skills(skills_str):
    return [skill.strip().lower() for skill in skills_str.split(",") if skill.strip()]

df_jobs = pd.read_csv("karkidi_jobs.csv")
df_jobs["SkillList"] = df_jobs["Skills"].fillna("").apply(clean_skills)
df_jobs["SkillString"] = df_jobs["SkillList"].apply(lambda x: " ".join(x))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import joblib

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df_jobs["SkillString"])

kmeans = KMeans(n_clusters=5, random_state=42)
df_jobs["Cluster"] = kmeans.fit_predict(X)

# Save models
joblib.dump(kmeans, "karkidi_kmeans.pkl")
joblib.dump(vectorizer, "karkidi_vectorizer.pkl")

df_jobs.to_csv("clustered_jobs.csv", index=False)

def classify_new_jobs(new_df, vectorizer, model):
    new_df["SkillList"] = new_df["Skills"].fillna("").apply(clean_skills)
    new_df["SkillString"] = new_df["SkillList"].apply(lambda x: " ".join(x))
    features = vectorizer.transform(new_df["SkillString"])
    new_df["Cluster"] = model.predict(features)
    return new_df

# Load model and vectorizer
model = joblib.load("karkidi_kmeans.pkl")
vectorizer = joblib.load("karkidi_vectorizer.pkl")

# Scrape fresh jobs and classify
new_jobs = scrape_karkidi_jobs(keyword="data science", pages=1)
classified = classify_new_jobs(new_jobs, vectorizer, model)

# Filter jobs by cluster of interest (example: Cluster 2)
user_interest = 2
matched = classified[classified["Cluster"] == user_interest]

print("\nJobs matching your preferred cluster:")
print(matched[["Title", "Company", "Location", "Skills", "Cluster"]])

