# -*- coding: utf-8 -*-
"""jobmonitoring system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13EUK_upanob3Ep_QfV8r6L8cz5uiKjaG
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be nice to the server

    return pd.DataFrame(jobs_list)

# Example use:
if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    print(df_jobs.head())

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# --- 1. Scraping Function ---
def scrape_karkidi_jobs(search_term="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{}/all/India?search={}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page, search_term.replace(' ', '%20'))
        print(f"Scraping page: {page} -> {url}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Skills": skills
                })
            except Exception as e:
                print("Parsing error:", e)
        time.sleep(1)

    return pd.DataFrame(jobs_list)

# --- 2. Preprocess Skills ---
def preprocess_skills(df):
    df['Skills'] = df['Skills'].fillna('').str.lower()
    return df

# --- 3. Vectorize Skills ---
def vectorize_skills(df):
    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(','), lowercase=True)
    skill_matrix = vectorizer.fit_transform(df['Skills'])
    return skill_matrix, vectorizer

# --- 4. Cluster Jobs ---
def cluster_jobs(skill_matrix, n_clusters=4):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(skill_matrix)
    return labels, kmeans

# --- 5. Recommend Jobs from Preferred Cluster ---
def recommend_jobs(df, cluster_labels, preferred_cluster):
    df['Cluster'] = cluster_labels
    matched_jobs = df[df['Cluster'] == preferred_cluster]
    print("\nJobs matching your preferred cluster:")
    print(matched_jobs[['Title', 'Company', 'Location', 'Skills', 'Cluster']])
    return matched_jobs

# --- Run the full pipeline ---
if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(search_term="data science", pages=2)
    df_jobs = preprocess_skills(df_jobs)
    skill_matrix, _ = vectorize_skills(df_jobs)
    labels, _ = cluster_jobs(skill_matrix, n_clusters=4)

    # You can adjust `preferred_cluster` (0, 1, 2, 3) based on your interest
    recommend_jobs(df_jobs, labels, preferred_cluster=2)

!pip install schedule

import schedule

def daily_job_check():
    df_new = scrape_karkidi_jobs(keyword="data science", pages=1)
    # Load model + vectorizer
    model = joblib.load("karkidi_kmeans_model.pkl")
    vect = joblib.load("karkidi_tfidf_vectorizer.pkl")
    X_new = vect.transform(df_new['Skills'].fillna(''))
    df_new['Cluster'] = model.predict(X_new)
    df_new.to_csv("daily_jobs.csv", index=False)
    print("Daily job check complete.")

# Schedule it every day at 9AM
schedule.every().day.at("09:00").do(daily_job_check)

while True:
    schedule.run_pending()